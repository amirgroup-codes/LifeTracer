{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3207dcfe",
   "metadata": {},
   "source": [
    "# Reproducing LifeTracer results\n",
    "\n",
    "## Overview\n",
    "\n",
    "**LifeTracer** is a comprehensive Python package for 2D gas chromatography analysis and molecular classification. This notebook provides a step-by-step guide to reproduce the results from our research paper, walking you through the complete pipeline from raw chromatographic data to trained classification models.\n",
    "\n",
    "### What you'll learn:\n",
    "- How to extract and process Total Ion Intensity (TII) heatmaps from raw data\n",
    "- Chromatographic peak detection and clustering\n",
    "- Parameter optimization using calibration datasets\n",
    "- Binary classification of samples using machine learning\n",
    "- Visualization of chromatographic data\n",
    "\n",
    "### System Requirements\n",
    "- **Disk Space**: Up to 500 GB for complete pipeline\n",
    "- **Memory**: Recommended 16+ GB RAM\n",
    "- **Python**: 3.10.8 or higher\n",
    "- **Environment**: Conda recommended\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d45ebd",
   "metadata": {},
   "source": [
    "## Quick Start\n",
    "\n",
    "**Option 1: Script Execution**\n",
    "You can run the complete pipeline using the scripts in the `scripts/` directory:\n",
    "\n",
    "```bash\n",
    "# Activate environment and run pipeline steps\n",
    "conda activate LifeTracer\n",
    "python scripts/step1_extract_heatmaps.py\n",
    "python scripts/step2_TII_alignment.py\n",
    "# ... continue with subsequent steps\n",
    "```\n",
    "\n",
    "**Option 2: Interactive Notebook**\n",
    "Follow the cells in this notebook for an interactive, step-by-step experience with detailed explanations.\n",
    "\n",
    "### Important Notes:\n",
    "- ‚ö†Ô∏è **Storage**: Steps 1-3 require at least **500 GB** of disk space.\n",
    "- üîß **Dependencies**: Ensure all paths in scripts point to correct project directory.\n",
    "- üîÑ **Flexibility**: Each step can be run independently or sequentially. If you want run run each step but you have not executed previous step, follow the required data download guide per step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2a782a",
   "metadata": {},
   "source": [
    "# Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b3a7822",
   "metadata": {},
   "source": [
    "## Linux/macOS Installation\n",
    "\n",
    "### Step 1: Create Environment\n",
    "```bash\n",
    "conda create -n LifeTracer python=3.10.8\n",
    "```\n",
    "\n",
    "### Step 2: Activate Environment\n",
    "```bash\n",
    "conda activate LifeTracer\n",
    "```\n",
    "\n",
    "### Step 3: Install Package\n",
    "```bash\n",
    "# Navigate to project directory\n",
    "cd LifeTracer\n",
    "\n",
    "# Install in development mode\n",
    "pip install -e .\n",
    "```\n",
    "\n",
    "### Step 4: Verify Installation\n",
    "```bash\n",
    "python -c \"import lifetracer; print('LifeTracer installed successfully!')\"\n",
    "```\n",
    "\n",
    "## Windows Installation\n",
    "\n",
    "The installation process is identical to Linux/macOS. Use Anaconda Prompt or PowerShell:\n",
    "\n",
    "```powershell\n",
    "# Follow the same steps as above\n",
    "conda create -n LifeTracer python=3.10.8\n",
    "conda activate LifeTracer\n",
    "cd LifeTracer\n",
    "pip install -e .\n",
    "```\n",
    "\n",
    "### Troubleshooting\n",
    "- **Permission Errors**: Use `pip install --user -e .`\n",
    "- **Environment Issues**: Try `conda clean --all` and recreate environment\n",
    "- **Path Problems**: Ensure you're in the correct project directory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eda344a",
   "metadata": {},
   "source": [
    "# Step 1: TII Extraction\n",
    "\n",
    "In this first step, we extract **Total Ion Intensity (TII)** images from raw chromatographic data for each sample and mass-to-charge ratio (m/z). This creates 2D heatmap representations that serve as the foundation for all subsequent analysis.\n",
    "\n",
    "### What happens in this step:\n",
    "1. **Input**: Raw CSV files containing chromatographic data\n",
    "2. **Process**: Convert 3D data (RT1, RT2, Intensity) to 2D heatmaps per m/z\n",
    "3. **Output**: TII heatmap files organized by sample and m/z value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39710137",
   "metadata": {},
   "source": [
    "## 1.1 M/Z Target List\n",
    "\n",
    "The m/z (mass-to-charge ratio) values we target for analysis are stored in `data/all_mz_values.csv`. This file contains m/z values ranging from **30 to 700**.\n",
    "\n",
    "> üí° **Tip**: You can modify this file to target specific m/z values relevant to your analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b70eab",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mThe kernel died. Error: ... View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Load and examine m/z values\n",
    "import pandas as pd\n",
    "\n",
    "mz_data = pd.read_csv('data/all_mz_values.csv')\n",
    "print(f\"Total M/Z values: {len(mz_data)}\")\n",
    "print(f\"Range: {mz_data['M/Z'].min()} - {mz_data['M/Z'].max()}\")\n",
    "print(\"\\nFirst 5 entries:\")\n",
    "mz_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f172be3e",
   "metadata": {},
   "source": [
    "## 1.2 Sample Labels & Metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67628242",
   "metadata": {},
   "source": [
    "Each sample requires metadata specification including the raw data filename, sample name, and classification label. This information is stored in `data/labels.csv`.\n",
    "\n",
    "### Label Convention:\n",
    "- **Label 1**: Terrestrial samples (soil, geological)\n",
    "- **Label 0**: Extraterrestrial samples (meteorites)\n",
    "- **Label -1**: Unlabeled samples (for prediction)\n",
    "\n",
    "> üí° **Tip**: For unlabeled samples that you want to classify, use label `-1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5db1233",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and examine sample labels\n",
    "labels_data = pd.read_csv('data/labels.csv')\n",
    "labels_data.head(18)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcdbc861",
   "metadata": {},
   "source": [
    "## 1.3 Data Download\n",
    "\n",
    "Download the raw chromatographic data required for Step 1.\n",
    "\n",
    "### ‚ö†Ô∏è Requirements:\n",
    "- **Disk Space**: ~350 GB free space\n",
    "- **Internet**: Stable connection for large file downloads\n",
    "- **Time**: 30-60 minutes depending on connection speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65868249",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Dataset URLs organized by type\n",
    "METEORITE_URLS = [\n",
    "    'https://huggingface.co/datasets/DS-20202/Meteorites_LifeTracer/resolve/main/230823_03_Murchison_Pristine_2.0_300uLDCM_100oC24h-003.csv',\n",
    "    'https://huggingface.co/datasets/DS-20202/Meteorites_LifeTracer/resolve/main/230830_01_EET96029_300uLDCM_100oC24h.csv',\n",
    "    'https://huggingface.co/datasets/DS-20202/Meteorites_LifeTracer/resolve/main/230830_02_Orgueil_300uLDCM_100oC24h-001.csv',\n",
    "    'https://huggingface.co/datasets/DS-20202/Meteorites_LifeTracer/resolve/main/230901_06_ALH83100_300uLDCM_100oC24h.csv',\n",
    "    'https://huggingface.co/datasets/DS-20202/Meteorites_LifeTracer/resolve/main/230901_07_LON94101_300uLDCM_100oC24h.csv',\n",
    "    'https://huggingface.co/datasets/DS-20202/Meteorites_LifeTracer/resolve/main/230901_08_LEW85311_300uLDCM_100oC24h.csv',\n",
    "    'https://huggingface.co/datasets/DS-20202/Meteorites_LifeTracer/resolve/main/231003_01_AZ_400uLDCM_100oC24h.csv',\n",
    "    'https://huggingface.co/datasets/DS-20202/Meteorites_LifeTracer/resolve/main/231003_02_Jbilet_Winselwan_300uLDCM_100oC24h.csv',\n",
    "]\n",
    "\n",
    "SOIL_URLS = [\n",
    "    'https://huggingface.co/datasets/DS-20202/SoilSample-LifeTracer/resolve/main/230823_01_Atacama_Soil_300uLDCM_100oC24h-001.csv',\n",
    "    'https://huggingface.co/datasets/DS-20202/SoilSample-LifeTracer/resolve/main/230823_02_Rio_Tinto_Soil_300uLDCM_100oC24h.csv',\n",
    "    'https://huggingface.co/datasets/DS-20202/SoilSample-LifeTracer/resolve/main/230823_04_Murchison_Soil_300uLDCM_100oC24h-001.csv',\n",
    "    'https://huggingface.co/datasets/DS-20202/SoilSample-LifeTracer/resolve/main/230823_05_Antarctica_Soil_300uLDCM_100oC24h-001.csv',\n",
    "    'https://huggingface.co/datasets/DS-20202/SoilSample-LifeTracer/resolve/main/230823_06_Jarosite_Soil_300uLDCM_100oC24h.csv',\n",
    "    'https://huggingface.co/datasets/DS-20202/SoilSample-LifeTracer/resolve/main/230823_07_Green_River_Shale_Soil_500uLDCM_100oC24h.csv',\n",
    "    'https://huggingface.co/datasets/DS-20202/SoilSample-LifeTracer/resolve/main/230901_05_GSFC_soil_300uLDCM_100oC24h.csv',\n",
    "    'https://huggingface.co/datasets/DS-20202/SoilSample-LifeTracer/resolve/main/230830_03_Lignite_300uLDCM_100oC24h-001.csv',\n",
    "    'https://huggingface.co/datasets/DS-20202/SoilSample-LifeTracer/resolve/main/231003_03_Utah_Soil_300uLDCM_100oC24h.csv',\n",
    "    'https://huggingface.co/datasets/DS-20202/SoilSample-LifeTracer/resolve/main/231003_04_Iceland_Soil_300uLDCM_100oC24h.csv'\n",
    "]\n",
    "\n",
    "# Create directory for raw data\n",
    "os.makedirs('downloads/raw', exist_ok=True)\n",
    "\n",
    "print(\"Downloading meteorite samples...\")\n",
    "for url in tqdm(METEORITE_URLS, desc=\"Meteorites\"):\n",
    "    filename = url.split('/')[-1]\n",
    "    urllib.request.urlretrieve(url, f'downloads/raw/{filename}')\n",
    "\n",
    "print(\"\\nDownloading soil samples...\")  \n",
    "for url in tqdm(SOIL_URLS, desc=\"Soil samples\"):\n",
    "    filename = url.split('/')[-1]\n",
    "    urllib.request.urlretrieve(url, f'downloads/raw/{filename}')\n",
    "\n",
    "print(f\"\\n‚úÖ Download complete! {len(METEORITE_URLS + SOIL_URLS)} files downloaded to downloads/raw/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63810b5",
   "metadata": {},
   "source": [
    "## 1.4 Execute TII Extraction\n",
    "\n",
    "### Method 1: Run via Script\n",
    "```bash\n",
    "python scripts/step1_extract_alignment.py\n",
    "```\n",
    "\n",
    "### Method 2: Interactive Execution (Current Notebook)\n",
    "Configure and run the extraction process directly in this notebook.\n",
    "\n",
    "### Configuration Parameters\n",
    "\n",
    "| Parameter | Description | Default |\n",
    "|-----------|-------------|---------|\n",
    "| `mz_list_path` | Path to M/Z values CSV | `data/all_mz_values.csv` |\n",
    "| `labels_path` | Path to sample labels CSV | `data/labels.csv` |\n",
    "| `m_z_column_name` | M/Z column in raw data | `M/Z` |\n",
    "| `area_column_name` | Intensity/Area column | `Area` |\n",
    "| `first_time_column_name` | First retention time (RT1) | `1st Time (s)` |\n",
    "| `second_time_column_name` | Second retention time (RT2) | `2nd Time (s)` |\n",
    "| `csv_file_name_column` | Raw CSV filename column | `csv_file_name` |\n",
    "| `label_column_name` | Sample label column | `label` |\n",
    "| `heatmap_dir` | Output directory for TIIs | `output/heatmaps/` |\n",
    "| `extract_heatmaps.raw_csv_path` | Raw data directory | `downloads/raw/` |\n",
    "| `extract_heatmaps.m_z_threshold` | M/Z quantization threshold | `0.5` |\n",
    "| `extract_heatmaps.parallel_processing` | Enable parallel processing | `True` |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14559ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lifetracer\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "# Configuration for TII extraction\n",
    "config = {\n",
    "    # Input data paths\n",
    "    \"mz_list_path\": \"data/all_mz_values.csv\",\n",
    "    \"labels_path\": \"data/labels.csv\",\n",
    "    \n",
    "    # Column mappings for raw CSV data\n",
    "    \"m_z_column_name\": \"M/Z\",\n",
    "    \"area_column_name\": \"Area\", \n",
    "    \"first_time_column_name\": \"1st Time (s)\",\n",
    "    \"second_time_column_name\": \"2nd Time (s)\",\n",
    "    \"csv_file_name_column\": \"csv_file_name\",\n",
    "    \"label_column_name\": \"label\",\n",
    "\n",
    "    # Output directory for TII heatmaps\n",
    "    \"heatmap_dir\": \"output/heatmaps/\",\n",
    "\n",
    "    # Extraction parameters\n",
    "    \"extract_heatmaps\": {\n",
    "        \"raw_csv_path\": \"downloads/raw/\",        # Path to raw data directory\n",
    "        \"m_z_threshold\": 0.5,                    # M/Z quantization threshold\n",
    "        \"parallel_processing\": True              # Enable parallel processing\n",
    "    },\n",
    "}\n",
    "\n",
    "# Create output directory\n",
    "Path(config[\"heatmap_dir\"]).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Starting TII extraction process...\")\n",
    "print(f\"Input directory: {config['extract_heatmaps']['raw_csv_path']}\")\n",
    "print(f\"Output directory: {config['heatmap_dir']}\")\n",
    "print(f\"Parallel processing: {config['extract_heatmaps']['parallel_processing']}\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Execute TII extraction\n",
    "lifetracer.extract_heatmap.heatmap_extraction(config)\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f\"\\n‚úÖ TII extraction completed in {elapsed_time/60:.1f} minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53203cd",
   "metadata": {},
   "source": [
    "# Step 2: TII Alignment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080ecf2b",
   "metadata": {},
   "source": [
    "We will perform this step from the paper:\n",
    "\n",
    "<div style=\"text-align: center;\"><img src=\"img/TII_Alignment.png\" width=\"800px\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f82d01",
   "metadata": {},
   "source": [
    "### Download data required for running step 2\n",
    "\n",
    "If did not do the previous step, you can download the processed data (unaligned TIIs).\n",
    "\n",
    "> üí° **Tip**:  Ensure you have `350 GB` of disk space available for this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13add779",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "import subprocess\n",
    "\n",
    "# Download all samples\n",
    "download_list = [\n",
    "    'https://huggingface.co/datasets/DS-20202/LifeTracer-Processed-Data/resolve/main/heatmaps.tar.gz.part-aa',\n",
    "    'https://huggingface.co/datasets/DS-20202/LifeTracer-Processed-Data/resolve/main/heatmaps.tar.gz.part-ab',\n",
    "    'https://huggingface.co/datasets/DS-20202/LifeTracer-Processed-Data/resolve/main/heatmaps.tar.gz.part-ac',\n",
    "    'https://huggingface.co/datasets/DS-20202/LifeTracer-Processed-Data/resolve/main/heatmaps.tar.gz.part-ad'\n",
    "]\n",
    "\n",
    "# create a folder for the raw data\n",
    "os.makedirs('output', exist_ok=True)\n",
    "\n",
    "# Download the data and store in output folder\n",
    "for url in download_list:\n",
    "    filename = url.split('/')[-1]\n",
    "    output_path = f'output/{filename}'\n",
    "    print(f\"Downloading {filename}...\")\n",
    "    urllib.request.urlretrieve(url, output_path)\n",
    "\n",
    "# Unzip the data\n",
    "print(\"Combining and extracting files...\")\n",
    "subprocess.run(\"cat output/heatmaps.tar.gz.part-a* > output/heatmaps.tar.gz && tar -xzf output/heatmaps.tar.gz -C output/\", shell=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ff3a00",
   "metadata": {},
   "source": [
    "### How to run the script\n",
    "\n",
    "You can run the cell bellow or run this command below:\n",
    "\n",
    "```bash\n",
    "python scripts/step2_extract_alignment.py\n",
    "```\n",
    "\n",
    "The description of the rest of the parameters are the same as step 1.\n",
    "| Parameter | Description |\n",
    "|-----------|-------------|\n",
    "| `heatmap_dir` | Input directory with TIIs from Step 1 |\n",
    "| `TII_aligned_dir` | Output directory for aligned TIIs |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99df8023",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"mz_list_path\": \"data/all_mz_values.csv\",\n",
    "    \"labels_path\": \"data/labels.csv\",\n",
    "    \"m_z_column_name\": \"M/Z\",\n",
    "    \"area_column_name\": \"Area\",\n",
    "    \"first_time_column_name\": \"1st Time (s)\",\n",
    "    \"second_time_column_name\": \"2nd Time (s)\",\n",
    "    \"csv_file_name_column\": \"csv_file_name\",\n",
    "    \"label_column_name\": \"label\",\n",
    "    \"heatmap_dir\": \"output/heatmaps/\",\n",
    "\n",
    "    # The aligned TIIs will be saved in this directory\n",
    "    \"TII_aligned_dir\": \"output/TII_aligned/\",\n",
    "}\n",
    "\n",
    "lifetracer.TII_alignment.align(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30100858",
   "metadata": {},
   "source": [
    "# Step 3: Calibration phase for parameter selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb04e9a",
   "metadata": {},
   "source": [
    "We will perform the parameter calibration procedure described in the paper here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "868c394d",
   "metadata": {},
   "source": [
    "### Calibration Dataset\n",
    "\n",
    "The calibration dataset contains expert-verified compounds, which will help us automate parameter selection.\n",
    "\n",
    "**Note:** In your experiments, you can modify `data/calibration_dataset.csv` with your own calibration data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1037a80a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Compound</th>\n",
       "      <th>RT1</th>\n",
       "      <th>RT2</th>\n",
       "      <th>Base Mass</th>\n",
       "      <th>ALH 83100</th>\n",
       "      <th>Aguas Zarcas</th>\n",
       "      <th>EET 96029</th>\n",
       "      <th>Jbilet Winselwan</th>\n",
       "      <th>Murchison</th>\n",
       "      <th>Orgueil</th>\n",
       "      <th>LEW 85311</th>\n",
       "      <th>LON 94101</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Naphthalene</td>\n",
       "      <td>4081.648</td>\n",
       "      <td>1.816</td>\n",
       "      <td>102</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Biphenyl</td>\n",
       "      <td>5178.400</td>\n",
       "      <td>1.728</td>\n",
       "      <td>154</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Phenanthrene</td>\n",
       "      <td>6692.130</td>\n",
       "      <td>2.040</td>\n",
       "      <td>178</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Anthracene</td>\n",
       "      <td>6727.170</td>\n",
       "      <td>2.019</td>\n",
       "      <td>178</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1-Phenylnaphthalene</td>\n",
       "      <td>6727.170</td>\n",
       "      <td>2.019</td>\n",
       "      <td>178</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>Acenaphthene</td>\n",
       "      <td>5626.910</td>\n",
       "      <td>1.600</td>\n",
       "      <td>153</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID             Compound       RT1    RT2  Base Mass  ALH 83100  \\\n",
       "0   1          Naphthalene  4081.648  1.816        102          1   \n",
       "1   2             Biphenyl  5178.400  1.728        154          1   \n",
       "2   3         Phenanthrene  6692.130  2.040        178          1   \n",
       "3   4           Anthracene  6727.170  2.019        178          0   \n",
       "4   5  1-Phenylnaphthalene  6727.170  2.019        178          0   \n",
       "5   6         Acenaphthene  5626.910  1.600        153          1   \n",
       "\n",
       "   Aguas Zarcas  EET 96029  Jbilet Winselwan  Murchison  Orgueil  LEW 85311  \\\n",
       "0             1          0                 1          1        1          1   \n",
       "1             0          0                 1          1        1          1   \n",
       "2             0          0                 0          1        1          1   \n",
       "3             0          0                 0          0        0          1   \n",
       "4             0          0                 0          0        0          1   \n",
       "5             1          0                 0          1        0          1   \n",
       "\n",
       "   LON 94101  \n",
       "0          1  \n",
       "1          1  \n",
       "2          1  \n",
       "3          0  \n",
       "4          0  \n",
       "5          1  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('data/calibration_dataset.csv').head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a9b724d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>M/Z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>153</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  M/Z\n",
       "0           0  102\n",
       "1           1  154\n",
       "2           2  178\n",
       "3           3  153"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This file contains the m/z values that we will use for calibration\n",
    "pd.read_csv('data/calibration_mz_values.csv').head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e89b7971",
   "metadata": {},
   "source": [
    "### Download data required for running step 3\n",
    "\n",
    "Run the cell below to download the required data.\n",
    "\n",
    "**Note:** Ensure you have `120 GB` of disk space available for this step.\n",
    "\n",
    "**Note:** Ensure that the file is unzipped in the output/TII_aligned/ directory to run the next step (or you can modify the path in the next step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d728cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "import subprocess\n",
    "\n",
    "# Download all samples\n",
    "download_list = [\n",
    "    'https://huggingface.co/datasets/DS-20202/LifeTracer-Processed-Data/resolve/main/TII_aligned.tar.gz.part-aa',\n",
    "    'https://huggingface.co/datasets/DS-20202/LifeTracer-Processed-Data/resolve/main/TII_aligned.tar.gz.part-ab',\n",
    "    'https://huggingface.co/datasets/DS-20202/LifeTracer-Processed-Data/resolve/main/TII_aligned.tar.gz.part-ac',\n",
    "    'https://huggingface.co/datasets/DS-20202/LifeTracer-Processed-Data/resolve/main/TII_aligned.tar.gz.part-ad',\n",
    "    'https://huggingface.co/datasets/DS-20202/LifeTracer-Processed-Data/resolve/main/TII_aligned.tar.gz.part-ae',\n",
    "    'https://huggingface.co/datasets/DS-20202/LifeTracer-Processed-Data/resolve/main/TII_aligned.tar.gz.part-af',\n",
    "]\n",
    "\n",
    "# create a folder for the raw data\n",
    "os.makedirs('output', exist_ok=True)\n",
    "\n",
    "# Download the data and store in downloads folder\n",
    "for url in download_list:\n",
    "    filename = url.split('/')[-1]\n",
    "    output_path = f'output/{filename}'\n",
    "    print(f\"Downloading {filename}...\")\n",
    "    urllib.request.urlretrieve(url, output_path)\n",
    "\n",
    "# Unzip the data\n",
    "part_files = ' '.join([f'output/TII_aligned.tar.gz.part-a{chr(ord(\"a\") + i)}' for i in range(6)])\n",
    "subprocess.run(f'cat {part_files} > output/TII_aligned.tar.gz', shell=True)\n",
    "subprocess.run('tar -xzf output/TII_aligned.tar.gz -C output/', shell=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167aed5c",
   "metadata": {},
   "source": [
    "### How to run the script\n",
    "\n",
    "You can run the cell bellow or run this command below:\n",
    "\n",
    "```bash\n",
    "python scripts/step3_calibration_phase.py\n",
    "```\n",
    "\n",
    "The descriptions of the remaining parameters are the same as in step 1.\n",
    "| Parameter | Description |\n",
    "|-----------|-------------|\n",
    "| `lambda1s` | The range of lambda1 values you want to explore. |\n",
    "| `lambda2s` | The range of lambda2 values you want to explore. |\n",
    "| `rt1_tol` | Maximum RT1 deviation of peak location for the reference compound. |\n",
    "| `rt2_tol` | Maximum RT2 deviation of peak location for the reference compound. |\n",
    "| `accuracy_threshold` | Filters out lambda1 and lambda2 combinations that result in less than `accuracy_threshold` in peak detection accuracy. |\n",
    "| `TII_aligned_dir` | Directory for aligned TIIs (from step 2). |\n",
    "| `calibration_phase_output_dir` | Directory for calibration phase outputs. |\n",
    "| `best_config_save_path` | Directory where you want the best configuration to be saved in JSON format. |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6836858f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lifetracer\n",
    "\n",
    "config = {\n",
    "    \"calibration_dataset_path\": \"data/calibration_dataset.csv\",\n",
    "    \"mz_list_path\": \"data/calibration_mz_values.csv\",\n",
    "    \"labels_path\": \"data/labels.csv\",\n",
    "    \"m_z_column_name\": \"M/Z\",\n",
    "    \"lambda1s\": [1,2,3,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20],\n",
    "    \"lambda2s\": [1,10,20,30,40,50,60,70,80,90,100,110,120,130,140,150,160,170,180,190,200],\n",
    "    \"rt1_tol\": 50,\n",
    "    \"rt2_tol\": 1,\n",
    "    \"accuracy_threshold\": 0.9,\n",
    "    \"TII_aligned_dir\": \"output/TII_aligned/\",\n",
    "    \"calibration_phase_output_dir\": \"output/calibration_phase/\",\n",
    "    \"best_config_save_path\": \"output/best_config/\",\n",
    "}\n",
    "\n",
    "lifetracer.calibration_phase.calibration_phase(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0d648178",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lambda1': 5, 'lambda2': 100, 'rt1_threshold': 50, 'rt2_threshold': 0.8}\n"
     ]
    }
   ],
   "source": [
    "# read json file\n",
    "import json\n",
    "with open('output/best_config/best_config.json', 'r') as f:\n",
    "    best_config = json.load(f)\n",
    "\n",
    "print(best_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3bfae3",
   "metadata": {},
   "source": [
    "So the best parameters are $\\lambda_1 = 5, \\lambda_2=100, RT1_{thrsh} = 50 s, RT1_{thrsh} = 0.8 s$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cafdee79",
   "metadata": {},
   "source": [
    "# Step 4: Extract peaks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e5afb9",
   "metadata": {},
   "source": [
    "In this step, we're going to extract peaks from aligned TIIs in step 3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa5c799",
   "metadata": {},
   "source": [
    "### Download data required for running step 4\n",
    "\n",
    "Run the cell below to download the required data.\n",
    "\n",
    "> üí° **Tip**: Ensure you have `120 GB` of disk space available for this step.\n",
    "\n",
    "> üí° **Tip**: Ensure that the file is unzipped in the output/TII_aligned/ directory to run the next step (or you can modify the path in the next step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6abb1b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "import subprocess\n",
    "\n",
    "# Download all samples\n",
    "download_list = [\n",
    "    'https://huggingface.co/datasets/DS-20202/LifeTracer-Processed-Data/resolve/main/TII_aligned.tar.gz.part-aa',\n",
    "    'https://huggingface.co/datasets/DS-20202/LifeTracer-Processed-Data/resolve/main/TII_aligned.tar.gz.part-ab',\n",
    "    'https://huggingface.co/datasets/DS-20202/LifeTracer-Processed-Data/resolve/main/TII_aligned.tar.gz.part-ac',\n",
    "    'https://huggingface.co/datasets/DS-20202/LifeTracer-Processed-Data/resolve/main/TII_aligned.tar.gz.part-ad',\n",
    "    'https://huggingface.co/datasets/DS-20202/LifeTracer-Processed-Data/resolve/main/TII_aligned.tar.gz.part-ae',\n",
    "    'https://huggingface.co/datasets/DS-20202/LifeTracer-Processed-Data/resolve/main/TII_aligned.tar.gz.part-af',\n",
    "]\n",
    "\n",
    "# create a folder for the raw data\n",
    "os.makedirs('output', exist_ok=True)\n",
    "\n",
    "# Download the data and store in downloads folder\n",
    "for url in download_list:\n",
    "    filename = url.split('/')[-1]\n",
    "    output_path = f'output/{filename}'\n",
    "    print(f\"Downloading {filename}...\")\n",
    "    urllib.request.urlretrieve(url, output_path)\n",
    "    print(f\"Downloaded {filename}\")\n",
    "\n",
    "# Unzip the data\n",
    "print(\"Combining and extracting files...\")\n",
    "subprocess.run(['cat', 'output/TII_aligned.tar.gz.part-aa', 'output/TII_aligned.tar.gz.part-ab', \n",
    "                'output/TII_aligned.tar.gz.part-ac', 'output/TII_aligned.tar.gz.part-ad',\n",
    "                'output/TII_aligned.tar.gz.part-ae', 'output/TII_aligned.tar.gz.part-af'], \n",
    "               stdout=open('output/TII_aligned.tar.gz', 'wb'))\n",
    "subprocess.run(['tar', '-xzf', 'output/TII_aligned.tar.gz'])\n",
    "print(\"Extraction completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec5740d",
   "metadata": {},
   "source": [
    "### How to run the script\n",
    "\n",
    "You can run the cell bellow or run this command below:\n",
    "\n",
    "```bash\n",
    "python scripts/step4_find_peaks.py\n",
    "```\n",
    "\n",
    "> üí° **Tip**: The best lambda1 and lambda2 was obtained from step 3.\n",
    "| Parameter | Description |\n",
    "|-----------|-------------|\n",
    "| `parallel_processing` | Enable/disable parallel processing |\n",
    "| `number_of_splits` | Number of splits for parallel processing |\n",
    "| `TII_aligned_dir` | Input directory with aligned TIIs |\n",
    "| `peaks_dir_path` | Output directory for detected peaks |\n",
    "| `lambda1` | Intensity threshold multiplier (default: 5) |\n",
    "| `lambda2` | Local intensity filter threshold (default: 100) |\n",
    "| `peak_max_neighbor_distance` | Max distance parameter for DBSCAN (default: 5) |\n",
    "| `strict_noise_filtering` | Enable/disable rigorous noise filtering |\n",
    "| `enable_noisy_regions` | Enable filtering of specific noisy regions |\n",
    "| `noisy_regions` | List of rectangular regions in (RT1, RT2) space to filter |\n",
    "| `convolution_filter.enable` | Enable/disable convolution-based filtering (Not used in the paper) |\n",
    "| `overall_filter.enable` | Enable/disable filtering based on non-zero pixels |\n",
    "| `overall_filter.non_zero_ratio_filter` | Threshold for non-zero pixel ratio |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8284f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lifetracer\n",
    "\n",
    "config = {\n",
    "    \"parallel_processing\": True,\n",
    "    \"number_of_splits\": 100,\n",
    "\n",
    "    \"mz_list_path\": \"data/all_mz_values.csv\",\n",
    "    \"labels_path\": \"data/labels.csv\",\n",
    "    \"m_z_column_name\": \"M/Z\",\n",
    "    \"area_column_name\": \"Area\",\n",
    "    \"first_time_column_name\": \"1st Time (s)\",\n",
    "    \"second_time_column_name\": \"2nd Time (s)\",\n",
    "    \"csv_file_name_column\": \"csv_file_name\",\n",
    "    \"label_column_name\": \"label\",\n",
    "    \"TII_aligned_dir\": \"output/TII_aligned/\",\n",
    "    \"peaks_dir_path\": \"output/peaks/\",\n",
    "    \"lambda1\": 5,\n",
    "    \"lambda2\": 100,\n",
    "    \"peak_max_neighbor_distance\": 5,\n",
    "    \"strict_noise_filtering\": True,\n",
    "\n",
    "    \"enable_noisy_regions\": True,\n",
    "    \"noisy_regions\": [\n",
    "        {\n",
    "            \"first_time_start\": 0,\n",
    "            \"second_time_start\": 0,\n",
    "            \"first_time_end\": -1,\n",
    "            \"second_time_end\": 1,\n",
    "            \"non_zero_ratio_region_threshold\": 1e-3\n",
    "        },\n",
    "        {\n",
    "            \"first_time_start\": 8700,\n",
    "            \"second_time_start\": 1.1,\n",
    "            \"first_time_end\": -1,\n",
    "            \"second_time_end\": 1.8,\n",
    "            \"non_zero_ratio_region_threshold\": 1e-2\n",
    "        },\n",
    "        {\n",
    "            \"first_time_start\": 8700,\n",
    "            \"second_time_start\": 0,\n",
    "            \"first_time_end\": -1,\n",
    "            \"second_time_end\": -1,\n",
    "            \"non_zero_ratio_region_threshold\": 1e-2\n",
    "        },\n",
    "        {\n",
    "            \"first_time_start\": 8690,\n",
    "            \"second_time_start\": 2.2,\n",
    "            \"first_time_end\": 8710,\n",
    "            \"second_time_end\": 3,\n",
    "            \"non_zero_ratio_region_threshold\": 1e-2\n",
    "        },\n",
    "        { # 202 EET\n",
    "            \"first_time_start\": 5174-50,\n",
    "            \"second_time_start\": 0,\n",
    "            \"first_time_end\": 5174 + 50,\n",
    "            \"second_time_end\": -1,\n",
    "            \"non_zero_ratio_region_threshold\": 1e-1\n",
    "        },\n",
    "\n",
    "        { # 202 EET\n",
    "            \"first_time_start\": 5300-50,\n",
    "            \"second_time_start\": 0,\n",
    "            \"first_time_end\": 5300 + 50,\n",
    "            \"second_time_end\": 1.8,\n",
    "            \"non_zero_ratio_region_threshold\": 1e-2\n",
    "        },\n",
    "\n",
    "        {\n",
    "            \"first_time_start\": 7700-50,\n",
    "            \"second_time_start\": 0,\n",
    "            \"first_time_end\": 7700 + 50,\n",
    "            \"second_time_end\": -1,\n",
    "            \"non_zero_ratio_region_threshold\": 1e-1\n",
    "        },\n",
    "        {\n",
    "            \"first_time_start\": 8700-50,\n",
    "            \"second_time_start\": 0,\n",
    "            \"first_time_end\": 8700 + 50,\n",
    "            \"second_time_end\": -1,\n",
    "            \"non_zero_ratio_region_threshold\": 1e-2\n",
    "        }\n",
    "    ],\n",
    "    \n",
    "    \"convolution_filter\": {\n",
    "        \"enable\": False,\n",
    "        \"lambda3\": 1000000,\n",
    "        \"rt1_window_size\": 100,\n",
    "        \"rt2_window_size\": 0.5,\n",
    "        \"rt1_stride\": 20,\n",
    "        \"rt2_stride\": 0.5,\n",
    "        \"non_zero_ratio_lambda3_filter\": 0.9\n",
    "    },\n",
    "\n",
    "    \"overall_filter\": {\n",
    "        \"enable\": True,\n",
    "        # \"lambda\": 10,\n",
    "        \"non_zero_ratio_filter\": 0.1\n",
    "    },\n",
    "}\n",
    "\n",
    "lifetracer.find_peaks.extract_peaks(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "395051a2",
   "metadata": {},
   "source": [
    "# Step 5: Create Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa756b5",
   "metadata": {},
   "source": [
    "In this step, we're going create features from the extracted peaks from step 4."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe66789",
   "metadata": {},
   "source": [
    "### Download data required for running step 5\n",
    "\n",
    "Run the cells below to download the required data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b297f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download TIIs\n",
    "import os\n",
    "import urllib.request\n",
    "import subprocess\n",
    "\n",
    "# Download all samples\n",
    "# If the cell doesn't run properly, you can download the data manually from the link below:\n",
    "download_list = [\n",
    "    'https://huggingface.co/datasets/DS-20202/LifeTracer-Processed-Data/resolve/main/TII_aligned.tar.gz.part-aa',\n",
    "    'https://huggingface.co/datasets/DS-20202/LifeTracer-Processed-Data/resolve/main/TII_aligned.tar.gz.part-ab',\n",
    "    'https://huggingface.co/datasets/DS-20202/LifeTracer-Processed-Data/resolve/main/TII_aligned.tar.gz.part-ac',\n",
    "    'https://huggingface.co/datasets/DS-20202/LifeTracer-Processed-Data/resolve/main/TII_aligned.tar.gz.part-ad',\n",
    "    'https://huggingface.co/datasets/DS-20202/LifeTracer-Processed-Data/resolve/main/TII_aligned.tar.gz.part-ae',\n",
    "    'https://huggingface.co/datasets/DS-20202/LifeTracer-Processed-Data/resolve/main/TII_aligned.tar.gz.part-af',\n",
    "]\n",
    "\n",
    "# create a folder for the raw data\n",
    "os.makedirs('output', exist_ok=True)\n",
    "\n",
    "# Download the data and store in downloads folder\n",
    "for url in download_list:\n",
    "    filename = url.split('/')[-1]\n",
    "    print(f\"Downloading {filename}...\")\n",
    "    urllib.request.urlretrieve(url, f'output/{filename}')\n",
    "    print(f\"Downloaded {filename}\")\n",
    "\n",
    "# Combine and extract the tar.gz parts\n",
    "print(\"Combining tar.gz parts...\")\n",
    "with open('output/TII_aligned.tar.gz', 'wb') as outfile:\n",
    "    for part in ['part-aa', 'part-ab', 'part-ac', 'part-ad', 'part-ae', 'part-af']:\n",
    "        with open(f'output/TII_aligned.tar.gz.{part}', 'rb') as infile:\n",
    "            outfile.write(infile.read())\n",
    "\n",
    "print(\"Extracting tar.gz file...\")\n",
    "import tarfile\n",
    "with tarfile.open('output/TII_aligned.tar.gz', 'r:gz') as tar:\n",
    "    tar.extractall('output/')\n",
    "print(\"Extraction complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e23944",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "import zipfile\n",
    "\n",
    "# Download all samples\n",
    "download_list = [\n",
    "    'https://huggingface.co/datasets/DS-20202/LifeTracer-Processed-Data/resolve/main/peaks.zip'\n",
    "]\n",
    "\n",
    "# create a folder for the raw data\n",
    "os.makedirs('output', exist_ok=True)\n",
    "\n",
    "# Download the data and store in downloads folder\n",
    "for url in download_list:\n",
    "    filename = url.split('/')[-1]\n",
    "    urllib.request.urlretrieve(url, f'output/{filename}')\n",
    "    print(f\"Downloaded {filename}\")\n",
    "\n",
    "# Unzip the data\n",
    "with zipfile.ZipFile('output/peaks.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall('output/')\n",
    "    print(\"Extracted peaks.zip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8725129c",
   "metadata": {},
   "source": [
    "### How to run the script\n",
    "\n",
    "You can run the cell bellow or run this command below:\n",
    "\n",
    "```bash\n",
    "python scripts/step5_retention_time_alignments.py\n",
    "```\n",
    "\n",
    "> üí° **Tip**: The best lambda1 and lambda2 was obtained from step 3.\n",
    "| Parameter | Description |\n",
    "|-----------|-------------|\n",
    "| `peaks_dir_path` | Path to directory containing peaks |\n",
    "| `features_path` | Output directory for features |\n",
    "| `rt1_threshold` | Maximum RT1 difference for clustering |\n",
    "| `rt2_threshold` | Maximum RT2 difference for clustering |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47fdeb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lifetracer\n",
    "\n",
    "config = {\n",
    "    \"parallel_processing\": True,\n",
    "    \"number_of_splits\": 100,\n",
    "\n",
    "    \"mz_list_path\": \"data/all_mz_values.csv\",\n",
    "    \"labels_path\": \"data/labels.csv\",\n",
    "    \"m_z_column_name\": \"M/Z\",\n",
    "    \"area_column_name\": \"Area\",\n",
    "    \"first_time_column_name\": \"1st Time (s)\",\n",
    "    \"second_time_column_name\": \"2nd Time (s)\",\n",
    "    \"csv_file_name_column\": \"csv_file_name\",\n",
    "    \"label_column_name\": \"label\",\n",
    "\n",
    "    \"features_path\": \"output/features/\",\n",
    "    \"TII_aligned_dir\": \"output/TII_aligned/\",\n",
    "    \"peaks_dir_path\": \"output/peaks/\",\n",
    "    \"lambda1\": [5], # best lambda1 from step 3\n",
    "    \"lambda2\": [100], # best lambda2 from step 3\n",
    "    \"rt1_threshold\": [50], # best rt1_threshold from step 3\n",
    "    \"rt2_threshold\": [0.8] # best rt2_threshold from step 3\n",
    "}\n",
    "\n",
    "lifetracer.retention_times_alignment.retention_times_alignment(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ced006",
   "metadata": {},
   "source": [
    "# Step 6: Parameter Selection for the Machine Learning Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d84865f",
   "metadata": {},
   "source": [
    "In this step, we're going to select the best hyperparameter for Logistic Regression with L2 regularization based on cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ddfa175",
   "metadata": {},
   "source": [
    "### Download data required for running step 6\n",
    "\n",
    "Run the cell below to download the required data.\n",
    "\n",
    "> üí° **Tip**: The data required for this step is lightweight.\n",
    "\n",
    "> üí° **Tip**: Ensure that the file is unzipped in the output/TII_aligned/ directory to run the next step (or you can modify the path in the next step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5f6cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "\n",
    "# Download all samples\n",
    "download_list = [\n",
    "    'https://huggingface.co/datasets/DS-20202/LifeTracer-Processed-Data/resolve/main/features.zip',\n",
    "    'https://huggingface.co/datasets/DS-20202/LifeTracer-Processed-Data/resolve/main/peaks.zip',\n",
    "]\n",
    "\n",
    "# create a folder for the raw data\n",
    "os.makedirs('output', exist_ok=True)\n",
    "\n",
    "# Download the data and store in output folder\n",
    "for url in download_list:\n",
    "    filename = url.split('/')[-1]\n",
    "    print(f\"Downloading {filename}...\")\n",
    "    urllib.request.urlretrieve(url, f\"output/{filename}\")\n",
    "    print(f\"Downloaded {filename}\")\n",
    "\n",
    "# Unzip the data\n",
    "import zipfile\n",
    "\n",
    "print(\"Extracting features.zip...\")\n",
    "with zipfile.ZipFile('output/features.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall('output/')\n",
    "\n",
    "print(\"Extracting peaks.zip...\")\n",
    "with zipfile.ZipFile('output/peaks.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall('output/')\n",
    "\n",
    "print(\"Extraction complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b2b5a98",
   "metadata": {},
   "source": [
    "### How to run the script\n",
    "\n",
    "You can run the cell bellow or run this command below:\n",
    "\n",
    "```bash\n",
    "python scripts/step6_parameters_selection.py\n",
    "```\n",
    "\n",
    "> üí° **Tip**: The best lambda1 and lambda2 was obtained from step 3.\n",
    "| Parameter | Description |\n",
    "|-----------|-------------|\n",
    "| `features_path` | Path to features |\n",
    "| `parameters_selection_path` | Output directory for selection results |\n",
    "| `C` | List of regularization strength values to test |\n",
    "| `seed` | Random seed for reproducibility |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c2376e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lifetracer\n",
    "\n",
    "config = {\n",
    "    \"mz_list_path\": \"data/all_mz_values.csv\",\n",
    "    \"labels_path\": \"data/labels.csv\",\n",
    "    \"m_z_column_name\": \"M/Z\",\n",
    "    \"area_column_name\": \"Area\",\n",
    "    \"first_time_column_name\": \"1st Time (s)\",\n",
    "    \"second_time_column_name\": \"2nd Time (s)\",\n",
    "    \"csv_file_name_column\": \"csv_file_name\",\n",
    "    \"label_column_name\": \"label\",\n",
    "\n",
    "    \"features_path\": \"output/features/\",\n",
    "    \"peaks_dir_path\": \"output/peaks/\",\n",
    "    \"parameters_selection_path\":\"output/parameters_selection/\",\n",
    "\n",
    "    # Logistic Regression with L2 regularization\n",
    "    \"C\": [1e-4,1e-3,1e-2,1e-1,1e0,1e+1,1e+2,1e+3,1e+4],\n",
    "    \"seed\": 42,\n",
    "    \"lambda1\": [5],\n",
    "    \"lambda2\": [100],\n",
    "    \"rt1_threshold\": [50],\n",
    "    \"rt2_threshold\": [0.8],\n",
    "}\n",
    "\n",
    "lifetracer.parameters_selection.parameters_selection(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "028beca6",
   "metadata": {},
   "source": [
    "# Step 7: Finally! Train a model on features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68cbbf6d",
   "metadata": {},
   "source": [
    "In this step, we're going to train a logistic regression model where the strength of regularization is determined from step 6."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155ae3fa",
   "metadata": {},
   "source": [
    "### Download data required for running step 7\n",
    "\n",
    "Run the cell below to download the required data.\n",
    "\n",
    "> üí° **Tip**: The data required for this step is lightweight.\n",
    "\n",
    "> üí° **Tip**: Ensure that the file is unzipped in the output/TII_aligned/ directory to run the next step (or you can modify the path in the next step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78ee4eac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wget\n",
      "  Downloading wget-3.2.zip (10 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: wget\n",
      "\u001b[33m  DEPRECATION: Building 'wget' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'wget'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001b[0m\u001b[33m\n",
      "\u001b[0m  Building wheel for wget (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9687 sha256=dfb6ca71d29b0199b05cb64307d82e98b035c492b55f1a9bfedad92a77da529b\n",
      "  Stored in directory: /Users/daniel/Library/Caches/pip/wheels/8b/f1/7f/5c94f0a7a505ca1c81cd1d9208ae2064675d97582078e6c769\n",
      "Successfully built wget\n",
      "Installing collected packages: wget\n",
      "Successfully installed wget-3.2\n"
     ]
    }
   ],
   "source": [
    "!pip install wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef87530f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading features.zip...\n",
      "Downloaded features.zip\n",
      "Downloading peaks.zip...\n",
      "Downloaded peaks.zip\n",
      "Extracting features.zip...\n",
      "Extracting peaks.zip...\n",
      "Extraction complete!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "\n",
    "# Download all samples\n",
    "download_list = [\n",
    "    'https://huggingface.co/datasets/DS-20202/LifeTracer-Processed-Data/resolve/main/features.zip',\n",
    "    'https://huggingface.co/datasets/DS-20202/LifeTracer-Processed-Data/resolve/main/peaks.zip',\n",
    "]\n",
    "\n",
    "# create a folder for the raw data\n",
    "os.makedirs('output', exist_ok=True)\n",
    "\n",
    "# Download the data and store in output folder\n",
    "for url in download_list:\n",
    "    filename = url.split('/')[-1]\n",
    "    print(f\"Downloading {filename}...\")\n",
    "    urllib.request.urlretrieve(url, f\"output/{filename}\")\n",
    "    print(f\"Downloaded {filename}\")\n",
    "\n",
    "# Unzip the data\n",
    "import zipfile\n",
    "\n",
    "print(\"Extracting features.zip...\")\n",
    "with zipfile.ZipFile('output/features.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall('output/')\n",
    "\n",
    "print(\"Extracting peaks.zip...\")\n",
    "with zipfile.ZipFile('output/peaks.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall('output/')\n",
    "\n",
    "print(\"Extraction complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2e8ecb",
   "metadata": {},
   "source": [
    "### How to run the script\n",
    "\n",
    "You can run the cell bellow or run this command below:\n",
    "\n",
    "```bash\n",
    "python scripts/step7_train_binary_classifier.py\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "300f2071",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-09-03 15:41:27.661\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlifetracer.src.binary_classifier\u001b[0m:\u001b[36mbinary_classifier\u001b[0m:\u001b[36m322\u001b[0m - \u001b[1mIntercept: [0.03806688]\u001b[0m\n",
      "\u001b[32m2025-09-03 15:41:49.474\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlifetracer.src.binary_classifier\u001b[0m:\u001b[36mbinary_classifier\u001b[0m:\u001b[36m346\u001b[0m - \u001b[1mSignatures saved.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder created here: output/lr_l2_results/top_features/\n",
      "Folder created here: output/lr_l2_results/feature_groups\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-09-03 15:41:51.197\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlifetracer.src.binary_classifier\u001b[0m:\u001b[36mbinary_classifier\u001b[0m:\u001b[36m373\u001b[0m - \u001b[1mRemoved 30 indices from top_feature_group_indices manually\u001b[0m\n",
      "\u001b[32m2025-09-03 15:41:54.519\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlifetracer.src.binary_classifier\u001b[0m:\u001b[36mbinary_classifier\u001b[0m:\u001b[36m385\u001b[0m - \u001b[1mTop 10 signatures plotted in top_coefficients folder.\u001b[0m\n",
      "\u001b[32m2025-09-03 15:41:54.944\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlifetracer.src.binary_classifier\u001b[0m:\u001b[36mbinary_classifier\u001b[0m:\u001b[36m389\u001b[0m - \u001b[1mPCA plot saved.\u001b[0m\n",
      "\u001b[32m2025-09-03 15:44:00.049\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlifetracer.src.binary_classifier\u001b[0m:\u001b[36mbinary_classifier\u001b[0m:\u001b[36m407\u001b[0m - \u001b[1m3D plot of signatures (png) saved.\u001b[0m\n",
      "\u001b[32m2025-09-03 15:44:00.917\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlifetracer.src.binary_classifier\u001b[0m:\u001b[36mbinary_classifier\u001b[0m:\u001b[36m411\u001b[0m - \u001b[1m3D interactive plot of peaks saved.\u001b[0m\n",
      "\u001b[32m2025-09-03 15:44:02.091\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlifetracer.src.binary_classifier\u001b[0m:\u001b[36mbinary_classifier\u001b[0m:\u001b[36m415\u001b[0m - \u001b[1m3D interactive plot of signatures saved.\u001b[0m\n",
      "\u001b[32m2025-09-03 15:44:02.441\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlifetracer.src.binary_classifier\u001b[0m:\u001b[36mbinary_classifier\u001b[0m:\u001b[36m418\u001b[0m - \u001b[1m3D plot of feature groups saved.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.472 0.04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-09-03 15:44:03.804\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlifetracer.src.binary_classifier\u001b[0m:\u001b[36mbinary_classifier\u001b[0m:\u001b[36m424\u001b[0m - \u001b[1mDistribution of peaks across m/z values saved.\u001b[0m\n",
      "\u001b[32m2025-09-03 15:44:03.809\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlifetracer.src.binary_classifier\u001b[0m:\u001b[36mmann_whitney_u_test_mz\u001b[0m:\u001b[36m110\u001b[0m - \u001b[1mMann Whitney U test for m/z p-value: 0.9999999966445333\u001b[0m\n",
      "\u001b[32m2025-09-03 15:44:03.810\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlifetracer.src.binary_classifier\u001b[0m:\u001b[36mmann_whitney_u_test_mz\u001b[0m:\u001b[36m115\u001b[0m - \u001b[1mFail to reject null hypothesis-> Abiotic peak distribution for m/z is not significantly lower than biotic\u001b[0m\n",
      "\u001b[32m2025-09-03 15:44:03.814\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlifetracer.src.binary_classifier\u001b[0m:\u001b[36mmann_whitney_u_test_rt1\u001b[0m:\u001b[36m123\u001b[0m - \u001b[1mMann Whitney U test for RT1 p-value: 6.465282194451019e-209\u001b[0m\n",
      "\u001b[32m2025-09-03 15:44:03.815\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlifetracer.src.binary_classifier\u001b[0m:\u001b[36mmann_whitney_u_test_rt1\u001b[0m:\u001b[36m126\u001b[0m - \u001b[1mReject null hypothesis-> Abiotic peak distribution for RT1 is significantly lower than biotic\u001b[0m\n",
      "\u001b[32m2025-09-03 15:44:03.819\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlifetracer.src.binary_classifier\u001b[0m:\u001b[36mmann_whitney_u_test_rt2\u001b[0m:\u001b[36m136\u001b[0m - \u001b[1mMann Whitney U test for RT2 p-value: 0.9999999998117325\u001b[0m\n",
      "\u001b[32m2025-09-03 15:44:03.820\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlifetracer.src.binary_classifier\u001b[0m:\u001b[36mmann_whitney_u_test_rt2\u001b[0m:\u001b[36m141\u001b[0m - \u001b[1mFail to reject null hypothesis-> Abiotic peak distribution for RT2 is not significantly higher than biotic\u001b[0m\n",
      "\u001b[32m2025-09-03 15:44:03.834\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlifetracer.src.binary_classifier\u001b[0m:\u001b[36mkolmogorov_smirnov_test\u001b[0m:\u001b[36m149\u001b[0m - \u001b[1mKolmogorov-Smirnov test for m/z - statistic: 0.1467191386019838, p-value: 1.0606304638923022e-20\u001b[0m\n",
      "\u001b[32m2025-09-03 15:44:03.835\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlifetracer.src.binary_classifier\u001b[0m:\u001b[36mkolmogorov_smirnov_test\u001b[0m:\u001b[36m152\u001b[0m - \u001b[1mReject null hypothesis -> The distributions of m/z for abiotic and biotic peaks are significantly different\u001b[0m\n",
      "\u001b[32m2025-09-03 15:44:03.884\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlifetracer.src.binary_classifier\u001b[0m:\u001b[36mkolmogorov_smirnov_test\u001b[0m:\u001b[36m149\u001b[0m - \u001b[1mKolmogorov-Smirnov test for RT1 - statistic: 0.45691332377524846, p-value: 1.1326092556406119e-203\u001b[0m\n",
      "\u001b[32m2025-09-03 15:44:03.885\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlifetracer.src.binary_classifier\u001b[0m:\u001b[36mkolmogorov_smirnov_test\u001b[0m:\u001b[36m152\u001b[0m - \u001b[1mReject null hypothesis -> The distributions of RT1 for abiotic and biotic peaks are significantly different\u001b[0m\n",
      "\u001b[32m2025-09-03 15:44:03.897\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlifetracer.src.binary_classifier\u001b[0m:\u001b[36mkolmogorov_smirnov_test\u001b[0m:\u001b[36m149\u001b[0m - \u001b[1mKolmogorov-Smirnov test for RT2 - statistic: 0.14881977158127785, p-value: 2.7510370654389267e-21\u001b[0m\n",
      "\u001b[32m2025-09-03 15:44:03.898\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlifetracer.src.binary_classifier\u001b[0m:\u001b[36mkolmogorov_smirnov_test\u001b[0m:\u001b[36m152\u001b[0m - \u001b[1mReject null hypothesis -> The distributions of RT2 for abiotic and biotic peaks are significantly different\u001b[0m\n",
      "\u001b[32m2025-09-03 15:44:10.557\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlifetracer.src.binary_classifier\u001b[0m:\u001b[36mplot_accuracy_drop_zeroing_coefficients\u001b[0m:\u001b[36m205\u001b[0m - \u001b[1mClassification accuracy remained above 90% after removing top 1773 features\u001b[0m\n",
      "\u001b[32m2025-09-03 15:44:10.558\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlifetracer.src.binary_classifier\u001b[0m:\u001b[36mplot_accuracy_drop_zeroing_coefficients\u001b[0m:\u001b[36m206\u001b[0m - \u001b[1mClassification accuracy remained above 80% after removing top 2099 features\u001b[0m\n",
      "\u001b[32m2025-09-03 15:44:10.759\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlifetracer.src.binary_classifier\u001b[0m:\u001b[36mbinary_classifier\u001b[0m:\u001b[36m437\u001b[0m - \u001b[1mAccuracy drop plot saved.\u001b[0m\n",
      "\u001b[32m2025-09-03 15:44:10.762\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlifetracer.src.binary_classifier\u001b[0m:\u001b[36mbinary_classifier\u001b[0m:\u001b[36m441\u001b[0m - \u001b[1mAccuracy before zeroing out the coefficients of the noisy feature groups: 1.0\u001b[0m\n",
      "\u001b[32m2025-09-03 15:44:10.876\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlifetracer.src.binary_classifier\u001b[0m:\u001b[36mbinary_classifier\u001b[0m:\u001b[36m461\u001b[0m - \u001b[1mAccuracy after zeroing out the coefficients of the noisy feature groups: 1.0\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import lifetracer\n",
    "\n",
    "config = {\n",
    "    \"mz_list_path\": \"data/all_mz_values.csv\",\n",
    "    \"labels_path\": \"data/labels.csv\",\n",
    "    \"m_z_column_name\": \"M/Z\",\n",
    "    \"area_column_name\": \"Area\",\n",
    "    \"first_time_column_name\": \"1st Time (s)\",\n",
    "    \"second_time_column_name\": \"2nd Time (s)\",\n",
    "    \"csv_file_name_column\": \"csv_file_name\",\n",
    "    \"label_column_name\": \"label\",\n",
    "\n",
    "    \"features_path\": \"output/features/\",\n",
    "    \"peaks_dir_path\": \"output/peaks/\",\n",
    "    \"results_dir\": \"output/lr_l2_results/\",\n",
    "\n",
    "    # Logistic Regression with l2 regularization\n",
    "    \"C\": 0.1, # best C from step 6\n",
    "    \"seed\": 42, \n",
    "    \"lambda1\": 5, # Selected from calibration step\n",
    "    \"lambda2\": 100, # Selected from calibration step\n",
    "    \"rt1_threshold\": 50, # Selected from calibration step\n",
    "    \"rt2_threshold\": 0.8, # Selected from calibration step\n",
    "}\n",
    "\n",
    "lifetracer.binary_classifier.binary_classifier(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25117188",
   "metadata": {},
   "source": [
    "# Plotting TIIs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b4c075",
   "metadata": {},
   "source": [
    "### Download required data\n",
    "\n",
    "Run the cell below to download the required data.\n",
    "\n",
    "> üí° **Tip**: Ensure you have `120 GB` of disk space available for this step.\n",
    "\n",
    "> üí° **Tip**: Ensure that the file is unzipped in the output/TII_aligned/ directory to run the next step (or you can modify the path in the next step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ff8687",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "import tarfile\n",
    "import subprocess\n",
    "\n",
    "# Download all samples\n",
    "download_list = [\n",
    "    'https://huggingface.co/datasets/DS-20202/LifeTracer-Processed-Data/resolve/main/TII_aligned.tar.gz.part-aa',\n",
    "    'https://huggingface.co/datasets/DS-20202/LifeTracer-Processed-Data/resolve/main/TII_aligned.tar.gz.part-ab',\n",
    "    'https://huggingface.co/datasets/DS-20202/LifeTracer-Processed-Data/resolve/main/TII_aligned.tar.gz.part-ac',\n",
    "    'https://huggingface.co/datasets/DS-20202/LifeTracer-Processed-Data/resolve/main/TII_aligned.tar.gz.part-ad',\n",
    "    'https://huggingface.co/datasets/DS-20202/LifeTracer-Processed-Data/resolve/main/TII_aligned.tar.gz.part-ae',\n",
    "    'https://huggingface.co/datasets/DS-20202/LifeTracer-Processed-Data/resolve/main/TII_aligned.tar.gz.part-af',\n",
    "]\n",
    "\n",
    "# create a folder for the raw data\n",
    "os.makedirs('output', exist_ok=True)\n",
    "\n",
    "# Download the data and store in output folder\n",
    "for url in download_list:\n",
    "    filename = url.split('/')[-1]\n",
    "    output_path = f'output/{filename}'\n",
    "    print(f\"Downloading {filename}...\")\n",
    "    urllib.request.urlretrieve(url, output_path)\n",
    "    print(f\"Downloaded {filename}\")\n",
    "\n",
    "# Concatenate the parts and extract\n",
    "print(\"Concatenating parts...\")\n",
    "with open('output/TII_aligned.tar.gz', 'wb') as outfile:\n",
    "    for i, suffix in enumerate(['aa', 'ab', 'ac', 'ad', 'ae', 'af']):\n",
    "        part_file = f'output/TII_aligned.tar.gz.part-{suffix}'\n",
    "        with open(part_file, 'rb') as infile:\n",
    "            outfile.write(infile.read())\n",
    "\n",
    "print(\"Extracting archive...\")\n",
    "with tarfile.open('output/TII_aligned.tar.gz', 'r:gz') as tar:\n",
    "    tar.extractall(path='output/')\n",
    "    \n",
    "print(\"Extraction complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87b5350",
   "metadata": {},
   "source": [
    "### How to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85b815a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lifetracer\n",
    "\n",
    "config = {\n",
    "    # Path to the CSV file containing the labels\n",
    "    \"labels_path\": \"data/labels.csv\",\n",
    "    \n",
    "    # Name of the column in the labels.csv that contains the labels\n",
    "    \"label_column_name\": \"label\",\n",
    "    \n",
    "    # Directory where generated heatmaps are stored\n",
    "    \"heatmap_dir\": \"output/TII_aligned/\",\n",
    "    \n",
    "    # Directory where generated plots will be saved\n",
    "    \"plot_dir\": \"output/plots/\",\n",
    "    \n",
    "    # Boolean flag indicating whether all samples should be processed\n",
    "    \"all_samples\": True, # Set to False to plot a single sample\n",
    "    \n",
    "    # Name of the sample to be analyzed if all_samples is False\n",
    "    \"sample_name\": \"230823_01_Atacama_Soil_300uLDCM_100oC24h-001.csv\",\n",
    "\n",
    "    \"csv_file_name_column\": \"csv_file_name\",\n",
    "    \n",
    "    # What m/z value to plot\n",
    "    \"m_z\": \"469\"\n",
    "}\n",
    "\n",
    "lifetracer.plot_heatmap.plot_heatmap(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6f2fb2",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8928dd72",
   "metadata": {},
   "source": [
    "### Download required data\n",
    "\n",
    "Run the cell below to download the required data (if you didn't run the previous steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ce5e1c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading features.zip...\n",
      "Downloaded features.zip\n",
      "Downloading peaks.zip...\n",
      "Downloaded peaks.zip\n",
      "Extracting features.zip...\n",
      "Extracting peaks.zip...\n",
      "Extraction complete!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "\n",
    "# Download all samples\n",
    "download_list = [\n",
    "    'https://huggingface.co/datasets/DS-20202/LifeTracer-Processed-Data/resolve/main/features.zip',\n",
    "    'https://huggingface.co/datasets/DS-20202/LifeTracer-Processed-Data/resolve/main/peaks.zip',\n",
    "]\n",
    "\n",
    "# create a folder for the raw data\n",
    "os.makedirs('output', exist_ok=True)\n",
    "\n",
    "# Download the data and store in output folder\n",
    "for url in download_list:\n",
    "    filename = url.split('/')[-1]\n",
    "    print(f\"Downloading {filename}...\")\n",
    "    urllib.request.urlretrieve(url, f\"output/{filename}\")\n",
    "    print(f\"Downloaded {filename}\")\n",
    "\n",
    "# Unzip the data\n",
    "import zipfile\n",
    "\n",
    "print(\"Extracting features.zip...\")\n",
    "with zipfile.ZipFile('output/features.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall('output/')\n",
    "\n",
    "print(\"Extracting peaks.zip...\")\n",
    "with zipfile.ZipFile('output/peaks.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall('output/')\n",
    "\n",
    "print(\"Extraction complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f0bcc3",
   "metadata": {},
   "source": [
    "### How to run the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "30070844",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-09-03 16:12:09.623\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlifetracer.src.evaluation\u001b[0m:\u001b[36meval\u001b[0m:\u001b[36m379\u001b[0m - \u001b[1mModel: lr_l2\u001b[0m\n",
      "\u001b[32m2025-09-03 16:12:09.626\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlifetracer.src.evaluation\u001b[0m:\u001b[36meval\u001b[0m:\u001b[36m380\u001b[0m - \u001b[1mStarting evaluation\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder created here: output/eval/lr_l2\n",
      "Parallel processing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed 741 - Processing parameters: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:03<00:00,  2.93it/s]\n",
      "Seed 627 - Processing parameters: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:03<00:00,  2.96it/s]\n",
      "Seed 661 - Processing parameters: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:03<00:00,  2.88it/s]\n",
      "Seed 522 - Processing parameters: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:03<00:00,  2.87it/s]\n",
      "Seed 137 - Processing parameters: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:02<00:00,  3.05it/s]\n",
      "Seed 860 - Processing parameters: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:02<00:00,  3.05it/s]\n",
      "Seed 412 - Processing parameters: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:03<00:00,  2.74it/s]\n",
      "Seed 514 - Processing parameters: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:03<00:00,  2.67it/s]\n",
      "Seed 738 - Processing parameters: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:03<00:00,  2.63it/s]\n",
      "Seed 679 - Processing parameters: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:03<00:00,  2.65it/s]\n",
      "Seed 741 - Processing parameters: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:03<00:00,  2.68it/s]\n",
      "Seed 522 - Processing parameters: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:03<00:00,  2.83it/s]\n",
      "Seed 514 - Processing parameters: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:03<00:00,  2.93it/s]\n",
      "Seed 661 - Processing parameters: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:03<00:00,  2.68it/s]\n",
      "Seed 627 - Processing parameters: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:03<00:00,  2.66it/s]\n",
      "Seed 137 - Processing parameters: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:03<00:00,  2.75it/s]\n",
      "Seed 679 - Processing parameters: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:02<00:00,  3.08it/s]\n",
      "Seed 738 - Processing parameters: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:03<00:00,  2.87it/s]\n",
      "Seed 412 - Processing parameters: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:03<00:00,  2.77it/s]\n",
      "Seed 860 - Processing parameters: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:03<00:00,  2.65it/s]\n",
      "Seed 741 - Processing parameters: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:02<00:00,  3.02it/s]\n",
      "Seed 522 - Processing parameters: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:03<00:00,  2.75it/s]\n",
      "Seed 627 - Processing parameters: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:03<00:00,  2.88it/s]\n",
      "Seed 412 - Processing parameters: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:03<00:00,  2.90it/s]\n",
      "Seed 679 - Processing parameters: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:03<00:00,  2.83it/s]\n",
      "Seed 514 - Processing parameters: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:03<00:00,  2.74it/s]\n",
      "Seed 661 - Processing parameters: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:03<00:00,  2.77it/s]\n",
      "Seed 137 - Processing parameters: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:03<00:00,  2.79it/s]\n",
      "Seed 738 - Processing parameters: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:03<00:00,  2.79it/s]\n",
      "Seed 860 - Processing parameters: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:03<00:00,  2.81it/s]\n",
      "Seed 741 - Processing parameters: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:02<00:00,  4.31it/s]\n",
      "Seed 412 - Processing parameters: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:01<00:00,  4.80it/s]\n",
      "Seed 627 - Processing parameters: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:01<00:00,  4.63it/s]\n",
      "Seed 738 - Processing parameters: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:01<00:00,  5.02it/s]\n",
      "Seed 522 - Processing parameters: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:02<00:00,  4.39it/s]\n",
      "Seed 514 - Processing parameters: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:02<00:00,  4.43it/s]\n",
      "Seed 679 - Processing parameters: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:02<00:00,  4.32it/s]\n",
      "Seed 860 - Processing parameters: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:01<00:00,  4.76it/s]\n",
      "Seed 661 - Processing parameters: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:02<00:00,  4.22it/s]\n",
      "Seed 137 - Processing parameters: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:02<00:00,  4.19it/s]\n",
      "Seed 741 - Processing parameters: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:02<00:00,  3.21it/s]\n",
      "Seed 627 - Processing parameters: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:02<00:00,  3.30it/s]\n",
      "Seed 412 - Processing parameters: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:02<00:00,  3.21it/s]\n",
      "Seed 738 - Processing parameters: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:02<00:00,  3.14it/s]\n",
      "Seed 514 - Processing parameters: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:02<00:00,  3.38it/s]\n",
      "Seed 522 - Processing parameters: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:02<00:00,  3.19it/s]\n",
      "Seed 860 - Processing parameters: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:02<00:00,  3.34it/s]\n",
      "Seed 137 - Processing parameters: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:02<00:00,  3.22it/s]\n",
      "Seed 661 - Processing parameters: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:02<00:00,  3.11it/s]\n",
      "Seed 679 - Processing parameters: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:02<00:00,  3.03it/s]\n",
      "Seed 412 - Processing parameters: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:02<00:00,  3.99it/s]\n",
      "Seed 741 - Processing parameters: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:02<00:00,  3.66it/s]\n",
      "Seed 738 - Processing parameters: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:02<00:00,  3.97it/s]\n",
      "Seed 627 - Processing parameters: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:02<00:00,  3.68it/s]\n",
      "Seed 514 - Processing parameters: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:02<00:00,  3.89it/s]\n",
      "Seed 522 - Processing parameters: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:02<00:00,  3.77it/s]\n",
      "Seed 679 - Processing parameters: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:02<00:00,  4.23it/s]\n",
      "Seed 661 - Processing parameters: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:02<00:00,  4.03it/s]\n",
      "Seed 860 - Processing parameters: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:02<00:00,  3.66it/s]\n",
      "Seed 137 - Processing parameters: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:02<00:00,  3.84it/s]\n",
      "Seed 412 - Processing parameters: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:02<00:00,  3.49it/s]\n",
      "Seed 522 - Processing parameters: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:02<00:00,  3.71it/s]\n",
      "Seed 741 - Processing parameters: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:02<00:00,  3.39it/s]\n",
      "Seed 679 - Processing parameters: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:02<00:00,  3.69it/s]\n",
      "Seed 738 - Processing parameters: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:02<00:00,  3.39it/s]\n",
      "Seed 661 - Processing parameters: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:02<00:00,  3.78it/s]\n",
      "Seed 627 - Processing parameters: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:02<00:00,  3.42it/s]\n",
      "Seed 860 - Processing parameters: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:02<00:00,  3.80it/s]\n",
      "Seed 514 - Processing parameters: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:02<00:00,  3.40it/s]\n",
      "Seed 137 - Processing parameters: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:02<00:00,  3.41it/s]\n",
      "Seed 412 - Processing parameters: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:02<00:00,  4.18it/s]\n",
      "Seed 522 - Processing parameters: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:02<00:00,  4.30it/s]\n",
      "Seed 661 - Processing parameters: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:02<00:00,  4.35it/s]\n",
      "Seed 738 - Processing parameters: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:02<00:00,  4.20it/s]\n",
      "Seed 860 - Processing parameters: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:02<00:00,  4.20it/s]\n",
      "Seed 741 - Processing parameters: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:02<00:00,  4.00it/s]\n",
      "Seed 679 - Processing parameters: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:02<00:00,  4.05it/s]\n",
      "Seed 627 - Processing parameters: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:02<00:00,  4.00it/s]\n",
      "Seed 514 - Processing parameters: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:02<00:00,  4.01it/s]\n",
      "Seed 137 - Processing parameters: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:02<00:00,  4.23it/s]\n",
      "Seed 514 - Processing parameters:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 8/9 [00:01<00:00,  3.94it/s]\n",
      "Seed 522 - Processing parameters: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:02<00:00,  4.02it/s]\n",
      "Seed 661 - Processing parameters: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:02<00:00,  3.98it/s]\n",
      "Seed 738 - Processing parameters: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:02<00:00,  3.95it/s]\n",
      "Seed 860 - Processing parameters: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:02<00:00,  3.98it/s]\n",
      "Seed 741 - Processing parameters: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:02<00:00,  4.01it/s]\n",
      "Seed 679 - Processing parameters: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:02<00:00,  4.01it/s]\n",
      "Seed 514 - Processing parameters: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:02<00:00,  4.18it/s]\n",
      "Seed 627 - Processing parameters: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:02<00:00,  4.08it/s]\n",
      "Seed 137 - Processing parameters: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:02<00:00,  4.38it/s]\n",
      "\u001b[32m2025-09-03 16:12:49.929\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlifetracer.src.evaluation\u001b[0m:\u001b[36meval\u001b[0m:\u001b[36m426\u001b[0m - \u001b[1mEvaluation finished\u001b[0m\n",
      "\u001b[32m2025-09-03 16:12:50.203\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlifetracer.src.evaluation\u001b[0m:\u001b[36mcalc_accuracy\u001b[0m:\u001b[36m354\u001b[0m - \u001b[1mavg validation acc: 91.1111111111111¬±0.6054026310473108\u001b[0m\n",
      "\u001b[32m2025-09-03 16:12:50.212\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlifetracer.src.evaluation\u001b[0m:\u001b[36mcalc_accuracy\u001b[0m:\u001b[36m355\u001b[0m - \u001b[1mavg test acc: 87.22222222222223¬±5.583264233956051\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[np.float64(88.88888888888889), np.float64(72.22222222222221), np.float64(88.88888888888889), np.float64(88.88888888888889), np.float64(88.88888888888889), np.float64(88.88888888888889), np.float64(94.44444444444444), np.float64(88.88888888888889), np.float64(83.33333333333334), np.float64(88.88888888888889)]\n",
      "[np.float64(20.78698548207745), np.float64(41.5739709641549), np.float64(20.78698548207745), np.float64(20.78698548207745), np.float64(20.78698548207745), np.float64(20.78698548207745), np.float64(15.713484026367725), np.float64(20.78698548207745), np.float64(23.570226039551585), np.float64(20.78698548207745)]\n"
     ]
    }
   ],
   "source": [
    "import lifetracer\n",
    "\n",
    "config = {\n",
    "    \"parallel_processing\": True,\n",
    "    \"mz_list_path\": \"data/all_mz_values.csv\",\n",
    "    \"labels_path\": \"data/labels.csv\",\n",
    "    \"m_z_column_name\": \"M/Z\",\n",
    "    \"area_column_name\": \"Area\",\n",
    "    \"first_time_column_name\": \"1st Time (s)\",\n",
    "    \"second_time_column_name\": \"2nd Time (s)\",\n",
    "    \"csv_file_name_column\": \"csv_file_name\",\n",
    "    \"label_column_name\": \"label\",\n",
    "\n",
    "    # Note: If you did not run all the steps, you can download the features and peaks from the huggingface.\n",
    "    # You can download the features and peaks from the huggingface. Follow the instructions in the notebook to download the data.\n",
    "    # Download features: https://huggingface.co/datasets/DS-20202/LifeTracer-Processed-Data/resolve/main/features.zip\n",
    "    # Download peaks: https://huggingface.co/datasets/DS-20202/LifeTracer-Processed-Data/resolve/main/peaks.zip\n",
    "    \"features_path\": \"output/features\", # Path to features directory\n",
    "    \"peaks_dir_path\": \"output/peaks\", # Path to peaks directory\n",
    "    \"eval_path\":\"output/eval/lr_l2\", # Change the path to your desired output directory\n",
    "    \n",
    "    \"model\": \"lr_l2\",\n",
    "    \"lr_l2\": {\n",
    "        \"C\": [1e-4,1e-3,1e-2,1e-1,1e0,1e+1,1e+2,1e+3,1e+4],\n",
    "        \"lambda1\": [5],\n",
    "        \"lambda2\": [100],\n",
    "        \"rt1_threshold\": [50],\n",
    "        \"rt2_threshold\": [0.8],\n",
    "    }\n",
    "\n",
    "    # Uncomment the model you want to evaluate\n",
    "\n",
    "    # \"model\": \"lr_l1\",\n",
    "    # \"lr_l1\": {\n",
    "    #     \"C\": [1e-4,1e-3,1e-2,1e-1,1e0,1e+1,1e+2,1e+3,1e+4],\n",
    "    #     \"lambda1\": [5],\n",
    "    #     \"lambda2\": [100],\n",
    "    #     \"rt1_threshold\": [50],\n",
    "    #     \"rt2_threshold\": [0.8],\n",
    "    # }\n",
    "\n",
    "    \n",
    "    # \"model\": \"svm\",\n",
    "    # \"svm\": {\n",
    "    #     \"C\": [1e-3,1e-2,1e-1,1e0,1e+1,1e+2,1e+3],\n",
    "    #     \"kernel\": [\"linear\",\"poly\",\"rbf\",\"sigmoid\"],\n",
    "    #     \"lambda1\": [5],\n",
    "    #     \"lambda2\": [100],\n",
    "    #     \"rt1_threshold\": [50],\n",
    "    #     \"rt2_threshold\": [0.8],\n",
    "    # },\n",
    "\n",
    "    # \"model\": \"rf\",\n",
    "    # \"rf\": {\n",
    "    #     \"n_estimators\": [20, 50, 100, 200, 500],\n",
    "    #     \"lambda1\": [5],\n",
    "    #     \"lambda2\": [100],\n",
    "    #     \"rt1_threshold\": [50],\n",
    "    #     \"rt2_threshold\": [0.8],\n",
    "    # }\n",
    "\n",
    "    # \"model\": 'NaiveBayes',\n",
    "    # \"NaiveBayes\": {\n",
    "    #     \"alpha\": [0.01, 0.1, 0.5, 1, 5, 10],\n",
    "    #     \"lambda1\": [5],\n",
    "    #     \"lambda2\": [100],\n",
    "    #     \"rt1_threshold\": [50],\n",
    "    #     \"rt2_threshold\": [0.8],\n",
    "    # }\n",
    "}\n",
    "\n",
    "lifetracer.evaluation.eval(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05fbf098",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LifeTracer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
